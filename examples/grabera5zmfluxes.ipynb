{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xesmf as xe\n",
    "import warnings\n",
    "import dask\n",
    "import sys\n",
    "\n",
    "from CASutils import calendar_utils as cal\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x2b1bf5ef92e8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "dask.config.set({'distributed.dashboard.link': 'https://jupyterhub.ucar.edu/dav/user/{USER}/proxy/{port}/status'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from dask_jobqueue import PBSCluster\n",
    "#from dask.distributed import Client\n",
    "#cluster = PBSCluster(cores=144, processes=144, memory='450GB', \n",
    "#                      resource_spec ='select=4:ncpus=36:mem=450GB', \n",
    "#                      walltime='02:00:00', queue='regular', \n",
    "#                      project='P04010022')\n",
    "#client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the workers going\n",
    "ncores = 1\n",
    "nworkers = 24\n",
    "nmems=\"25GB\"\n",
    "from dask.distributed import Client\n",
    "from dask_jobqueue import SLURMCluster\n",
    "cluster = SLURMCluster(\n",
    "cores = ncores, processes = ncores, project=\"P04010022\", walltime=\"04:00:00\")\n",
    "cluster.scale(nworkers)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the workers going\n",
    "#ncores = 36\n",
    "#nmem = '460GB'\n",
    "#nmem = str(int(365*ncores/36))+'GB'\n",
    "#from dask_jobqueue import SLURMCluster\n",
    "#from dask.distributed import Client\n",
    "#cluster = SLURMCluster(cores=ncores,\n",
    "#                     processes=ncores, memory=nmem,\n",
    "#                     project='P04010022',\n",
    "#                     walltime='12:00:00')\n",
    "#cluster.scale(ncores)\n",
    "#client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='background-color: #f2f2f2; display: inline-block; padding: 10px; border: 1px solid #999999;'>\n",
       "  <h3>SLURMCluster</h3>\n",
       "  <ul>\n",
       "    <li><b>Dashboard: </b><a href='https://jupyterhub.ucar.edu/dav/user/islas/proxy/39864/status' target='_blank'>https://jupyterhub.ucar.edu/dav/user/islas/proxy/39864/status</a>\n",
       "  </ul>\n",
       "</div>\n"
      ],
      "text/plain": [
       "SLURMCluster('tcp://10.12.205.27:33582', workers=0, threads=0, memory=0 B)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "concurrent.futures._base.CancelledError\n"
     ]
    }
   ],
   "source": [
    "#cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.12.205.27:33582</li>\n",
       "  <li><b>Dashboard: </b><a href='https://jupyterhub.ucar.edu/dav/user/islas/proxy/39864/status' target='_blank'>https://jupyterhub.ucar.edu/dav/user/islas/proxy/39864/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>21</li>\n",
       "  <li><b>Cores: </b>21</li>\n",
       "  <li><b>Memory: </b>525.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.12.205.27:33582' processes=1 threads=1, memory=25.00 GB>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do this until you see you've got some workers\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of ERA5 data on RDA\n",
    "filepath=\"/gpfs/fs1/collections/rda/data/ds633.0/e5.oper.an.pl/\"\n",
    "# output location\n",
    "outpath=\"/glade/scratch/islas/processed/era5/TEMdiags/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ystart=1993 ; yend=1993 ; nyears=yend-ystart+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open up CESM data to get the output grid.\n",
    "cesmdat = xr.open_dataset(\"/glade/campaign/cesm/collections/cesmLE/CESM-CAM5-BGC-LE/atm/proc/tseries/monthly/PHIS/f.e11.F1850C5CNTVSST.f09_f09.002.cam.h0.PHIS.040101-050012.nc\")\n",
    "grid_out = xr.Dataset({'lat': (['lat'], cesmdat.lat)}, {'lon': (['lon'], cesmdat.lon)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1993-12\n",
      "using dimensions ('latitude', 'longitude') from data variable T as the horizontal dimensions for this dataset.\n",
      "CPU times: user 21.6 s, sys: 12.5 s, total: 34 s\n",
      "Wall time: 7min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "reusewgt=True\n",
    "wgtfile=outpath+\"wgtfile.nc\"\n",
    "for iyear in range(ystart,yend+1,1):\n",
    "    #for imon in range(1,13,1):\n",
    "    for imon in range(12,13,1):\n",
    "        monstr=str(imon).zfill(2)\n",
    "        print(str(iyear)+\"-\"+monstr)\n",
    "        \n",
    "        ds = xr.open_mfdataset(\n",
    "        filepath + str(iyear)+monstr+ \"/*_[u, v, w, t].*.nc\",\n",
    "        coords=\"minimal\",\n",
    "        join=\"override\",\n",
    "        decode_times=True,\n",
    "        use_cftime=True,\n",
    "        chunks={\"time\":24, \"level\":15},\n",
    "        parallel=True,\n",
    "        data_vars = \"minimal\",\n",
    "        compat = \"override\")\n",
    "        \n",
    "        ds = ds.set_coords(\"utc_date\")\n",
    "        \n",
    "        dayspermon = ds.time.dt.daysinmonth.data\n",
    "        dayendstr = str(dayspermon[0])\n",
    "        timeout = pd.date_range(\n",
    "            start=str(iyear)+\"-\"+monstr+\"-01\",\n",
    "            end=str(iyear)+\"-\"+monstr+\"-\"+dayendstr)\n",
    "        \n",
    "        ds = ds.loc[{\"time\": ds.time.dt.hour.isin([0,6,12,18])}]\n",
    "             \n",
    "        ds[\"T\"] = ds.T * (ds.T.level / 1000.0)**(-2./7.)\n",
    "              \n",
    "        regridder = xe.Regridder(ds.U, grid_out, 'bilinear', periodic=True, \n",
    "                                 reuse_weights=reusewgt, filename=wgtfile)\n",
    "        reusewgt = True\n",
    "        \n",
    "        regridded = regridder(ds).persist()\n",
    "        \n",
    "        # this was causing issues before.  Either need to compute the zonal mean before\n",
    "        # subtracting or do the persist on the regridded above.\n",
    "        zonal_means = regridded.mean(\"lon\")\n",
    "        anomaly = regridded - zonal_means\n",
    "        \n",
    "        anomaly['uv'] = anomaly.U*anomaly.V\n",
    "        anomaly['vt'] = anomaly.V*anomaly.T\n",
    "        anomaly['uw'] = anomaly.U*anomaly.W\n",
    "        \n",
    "        zonal_means = zonal_means.merge(anomaly[['uv','vt','uw']].mean(\"lon\"))\n",
    "        \n",
    "        temdiags = zonal_means.rename(\n",
    "            {\n",
    "                \"uv\":\"UVzm\",\n",
    "                \"vt\":\"VTHzm\",\n",
    "                \"uw\":\"UWzm\",\n",
    "                \"U\":\"Uzm\",\n",
    "                \"V\":\"Vzm\",\n",
    "                \"W\":\"Wzm\",\n",
    "                \"T\":\"THzm\",\n",
    "            })\n",
    "\n",
    "        temdiags = temdiags.groupby('time.dayofyear').mean()\n",
    "        temdiags = temdiags.rename({'dayofyear':'time'})\n",
    "        temdiags['time'] == timeout\n",
    "        outfile = outpath+\"fluxes_\"+str(iyear)+\"-\"+monstr+\".nc\"\n",
    "        # Since temdiags is small, it's better to load it and write in serial than in\n",
    "        # parallel to netcdf.\n",
    "        temdiags.load().to_netcdf(path=outfile)\n",
    "        \n",
    "        client.cancel(regridded) # deleted from distriubted RAM to recover memory\n",
    "        ds.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-ecpaperenv]",
   "language": "python",
   "name": "conda-env-miniconda3-ecpaperenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
